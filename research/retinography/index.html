
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="index, follow" />

  <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="../../theme/stylesheet/style.min.css">

    <link id="dark-theme-style" rel="stylesheet" type="text/css"
          media="(prefers-color-scheme: dark)"
    href="../../theme/stylesheet/dark-theme.min.css">

    <link id="pygments-dark-theme" rel="stylesheet" type="text/css"
              media="(prefers-color-scheme: dark)"
          href="../../theme/pygments/solarized-dark.min.css">
    <link id="pygments-light-theme" rel="stylesheet" type="text/css"
              media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"
          href="../../theme/pygments/solarized-light.min.css">



  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha512-z3gLpd7yknf1YoNbCzqRKc4qyor8gaKU1qmn+CShxbuBusANI9QpRohGBreCFkKxLhei6S9CQXFEbbKuqLg0DA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.4/css/academicons.min.css" integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin="anonymous" referrerpolicy="no-referrer" />

  <link rel="stylesheet" type="text/css" href="../../css/custom.css">

  <link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon">
  <link rel="icon" href="/images/favicon.ico" type="image/x-icon">
  <link rel="apple-touch-icon" href="/images/profile_128.png">

  <!-- Chrome, Firefox OS and Opera -->
  <meta name="theme-color" content="#333">
  <!-- Windows Phone -->
  <meta name="msapplication-navbutton-color" content="#333">
  <!-- iOS Safari -->
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <!-- Microsoft EDGE -->
  <meta name="msapplication-TileColor" content="#333">









    <link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/images/favicon.ico" type="image/x-icon">



    <!-- Chrome, Firefox OS and Opera -->
    <meta name="theme-color" content="#333">
    <!-- Windows Phone -->
    <meta name="msapplication-navbutton-color" content="#333">
    <!-- iOS Safari -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Microsoft EDGE -->
    <meta name="msapplication-TileColor" content="#333">


<meta name="author" content="André Anjos" />
<meta name="description" content="Semantical segmentation of eye fundus structures, and disease detection from retinography, play a key role in mass screening using this tecnology. Despite the incredible progress in these fields, the lack of annotated images (due to cost), and rigor in the comparison of trained models has led to the conclusion larger and more dense network models provide more accurate results for such tasks. We present our findings on different architectures and databases in this context." />
<meta name="keywords" content="">


  <meta property="og:site_name" content="André Anjos"/>
  <meta property="og:title" content="Semantical Segmentation: Retinography"/>
  <meta property="og:description" content="Semantical segmentation of eye fundus structures, and disease detection from retinography, play a key role in mass screening using this tecnology. Despite the incredible progress in these fields, the lack of annotated images (due to cost), and rigor in the comparison of trained models has led to the conclusion larger and more dense network models provide more accurate results for such tasks. We present our findings on different architectures and databases in this context."/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="../../research/retinography/"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2021-04-14 12:30:00+02:00"/>
  <meta property="article:modified_time" content=""/>
  <meta property="article:author" content="../../author/andre-anjos.html">
  <meta property="article:section" content="research"/>
  <meta property="og:image" content="../../images/pictures/retinography-vessels-segmented.jpg">

  <title>André Anjos &ndash; Semantical Segmentation: Retinography</title>


</head>
<body >

<aside>
  <div>
    <a href="../../">
      <img src="/images/profile_128.png" alt="André Anjos" title="André Anjos">
    </a>

    <h1>
      <a href="../../">André Anjos</a>
    </h1>

    <p>Machine Learning, Computer Vision, Medical Imaging & Health Data Analysis, Reproducibility, Ph.D.</p>


    <nav>
      <ul class="list">



          <li>
            <a target="_self" href="https://anjos.ai/cv/cv.pdf" >CV</a>
          </li>
          <li>
            <a target="_self" href="/research/" >Research</a>
          </li>
          <li>
            <a target="_self" href="/publications/" >Publications</a>
          </li>
          <li>
            <a target="_self" href="/theses/" >Supervised Students</a>
          </li>
          <li>
            <a target="_self" href="/courses/" >Courses</a>
          </li>
          <li>
            <a target="_self" href="/software/" >Software</a>
          </li>
          <li>
            <a target="_self" href="/media/" >Talks & Media</a>
          </li>
          <li>
            <a target="_self" href="/about/" >About</a>
          </li>
          <li>
            <a target="_self" href="/short-bio/" >Short Bio</a>
          </li>
          <li>
            <a target="_self" href="/contact/" >Contact</a>
          </li>
      </ul>
    </nav>

    <ul class="social">
      <li>
        <a class="sc-orcid"
           href="https://orcid.org/0000-0001-7248-4014"
           target="_blank">
<i class="fa-brands fa-orcid"></i>        </a>
      </li>
      <li>
        <a class="sc-google-scholar"
           href="https://scholar.google.ch/citations?user=pAfLhMoAAAAJ"
           target="_blank">
<i class="ai ai-google-scholar-square"></i>        </a>
      </li>
      <li>
        <a class="sc-linkedin"
           href="https://www.linkedin.com/in/andreranjos/"
           target="_blank">
<i class="fa-brands fa-linkedin"></i>        </a>
      </li>
      <li>
        <a class="sc-stack-overflow"
           href="https://stackoverflow.com/users/712525/andré-anjos"
           target="_blank">
<i class="fa-brands fa-stack-overflow"></i>        </a>
      </li>
      <li>
        <a class="sc-github"
           href="https://github.com/anjos"
           target="_blank">
<i class="fa-brands fa-github"></i>        </a>
      </li>
      <li>
        <a class="sc-gitlab"
           href="https://gitlab.idiap.ch/bob"
           target="_blank">
<i class="fa-brands fa-gitlab"></i>        </a>
      </li>
    </ul>
  </div>

</aside>
  <main>


<article class="single">
  <header>
      
    <h1 id="retinography">Semantical Segmentation: Retinography</h1>
    <p>
      Posted on Wed 14 April 2021 in <a href="../../research/">research</a>

    </p>
  </header>


  <div>
    <p class="figure center"><img alt="Original image from the HRF dataset" class="align-middle" src="../../images/pictures/retinography.jpg" style="height: 220px;" /> <img alt="Image with vessels segmented" class="align-middle" src="../../images/pictures/retinography-vessels-segmented.jpg" style="height: 220px;" /></p>
<p class="caption figure center"><em>Predicted vessel maps vs. ground truths for a DL model evaluated on the
high-resolution HRF test-set. True positives, false positives and false
negatives are displayed in green, blue and red respectively.</em></p>
<div class="admonition note">
<p class="first admonition-title">Reproducibility Checklist</p>
<ul class="simple">
<li>All datasets used in this study are public</li>
<li><a class="reference external" href="https://gitlab.idiap.ch/bob/bob.ip.binseg">Software is open-source</a>,
extensible and tested on a regular basis</li>
<li><a class="reference external" href="https://www.idiap.ch/software/bob/docs/bob/bob.ip.binseg/master/index.html">Complete documentation is available</a></li>
</ul>
<p><strong>Partnership</strong></p>
<p class="last">Part of this study was conducted in partnership with the Department of
Ophthalmology at University Hospital of Grenoble Alpes.</p>
</div>
<p>Since the introduction of U-Nets in 2015, the field of medical image
segmentation has seen renewed interest bringing in a variety of fully
convolutional (deep) neural network (FCN) architectures for binary and
multi-class segmentation problems promising very attractive results, with
applications in computed tomography, retinography, and histopathology to cite a
few.  Despite the incredible progress, the lack of annotated images (due to
cost), and rigor in the comparison of trained models has led to the conclusion
larger and more dense network models provide more accurate results for this
task.  This is particularly noticeable in ophtalmological images such as those
from bi-dimensional eye fundus photography (retinography).  While retinography
is not used for precision diagnostics, it remains relatively cheap and very
effective means for mass screening.  Semantical segmentation of eye fundus
structures plays a key role in this process.</p>
<p>We tried to address these gaps in two different ways.  The
first <a href='#arxiv-2019' id='ref-arxiv-2019-1'>(Laibacher and Anjos, 2019)</a> was to conduct and publish rigorous (open source,
reproducible) benchmarks with popular retinography datasets and
state-of-the-art FCN models in which we: i) showed that simple transformation
techniques like rescaling, padding and cropping of combined lower-resolution
source datasets to the resolution and spatial composition of a
higher-resolution target dataset can be a surprisingly effective way to improve
segmentation quality in unseen conditions; ii) we proposed a set of plots and
metrics that give additional insights into model performance and demonstrated
via tables and plots how to take advantage of that information, throwing a new
light over some published benchmarks.  We argue the performance of many
contributions available in literature is actually quite comparable within
standard deviation margins of each other, in spite of huge differences in the
number of parameters for different architectures.  Finally, <a class="reference external" href="https://gitlab.idiap.ch/bob/bob.ip.binseg">we made our
findings reproducible</a>, distributing code and documentation for future
researchers to build upon, in the hopes to inspire future work in the
field.</p>
<p>In a second contribution <a href='#arxiv-2020' id='ref-arxiv-2020-1'>(Galdran et al., 2020)</a> we propose that a minimalistic version
of a standard U-Net with 3 orders of magnitude less parameters, carefully
trained and rigorously evaluated, closely approximates the state-of-the-art
performance in vessel segmentation for retinography.  In addition, we propose a
simple extension, dubbed W-Net, by concatenating two U-Nets together, which
reaches outstanding performance on several popular datasets, still using orders
of magnitude less learnable weights than any previously published approach.
This work also provide a very comprehensive intra and cross-dataset performance
analysis, involving up to 10 different databases, including artery/vein
multi-class semantic segmentation.</p>
<!-- links here: -->
<hr>
<h2>Bibliography</h2>
<p id='arxiv-2020'>Adrian Galdran, André Anjos, José Dolz, Hadi Chakor, Hervé Lombaert, and Ismail&nbsp;Ben Ayed.
The little w-net that could: state-of-the-art retinal vessel segmentation with minimalistic models.
September 2020.
URL: <a href="https://arxiv.org/abs/2009.01907">https://arxiv.org/abs/2009.01907</a>, <a href="https://arxiv.org/abs/2009.01907">arXiv:2009.01907</a>. <a class="cite-backref" href="#ref-arxiv-2020-1" title="Jump back to reference 1">↩</a></p>
<p id='arxiv-2019'>Tim Laibacher and André Anjos.
On the evaluation and real-world usage scenarios of deep vessel segmentation for retinography.
September 2019.
URL: <a href="https://arxiv.org/abs/1909.03856">https://arxiv.org/abs/1909.03856</a>, <a href="https://arxiv.org/abs/1909.03856">arXiv:1909.03856</a>. <a class="cite-backref" href="#ref-arxiv-2019-1" title="Jump back to reference 1">↩</a></p>

  </div>
  <div class="tag-cloud">
    <p>
    </p>
  </div>






</article>

<footer>
<p>&copy; André Anjos 2023</p>
</footer>  </main>

<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " André Anjos ",
  "url" : "../..",
  "image": "/images/profile_128.png",
  "description": "Professional Website"
}
</script>
</body>
</html>